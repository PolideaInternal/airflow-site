:mod:`airflow.gcp.operators.datastore`
======================================

.. py:module:: airflow.gcp.operators.datastore

.. autoapi-nested-parse::

   This module contains Google Datastore operators.



Module Contents
---------------

.. py:class:: DatastoreExportOperator(bucket:str, namespace:Optional[str]=None, datastore_conn_id:str='google_cloud_default', cloud_storage_conn_id:str='google_cloud_default', delegate_to:Optional[str]=None, entity_filter:Optional[dict]=None, labels:Optional[dict]=None, polling_interval_in_seconds:int=10, overwrite_existing:bool=False, project_id:Optional[str]=None, *args, **kwargs)

   Bases: :class:`airflow.models.BaseOperator`

   Export entities from Google Cloud Datastore to Cloud Storage

   :param bucket: name of the cloud storage bucket to backup data
   :type bucket: str
   :param namespace: optional namespace path in the specified Cloud Storage bucket
       to backup data. If this namespace does not exist in GCS, it will be created.
   :type namespace: str
   :param datastore_conn_id: the name of the Datastore connection id to use
   :type datastore_conn_id: str
   :param cloud_storage_conn_id: the name of the cloud storage connection id to
       force-write backup
   :type cloud_storage_conn_id: str
   :param delegate_to: The account to impersonate, if any.
       For this to work, the service account making the request must have domain-wide
       delegation enabled.
   :type delegate_to: str
   :param entity_filter: description of what data from the project is included in the
       export, refer to
       https://cloud.google.com/datastore/docs/reference/rest/Shared.Types/EntityFilter
   :type entity_filter: dict
   :param labels: client-assigned labels for cloud storage
   :type labels: dict
   :param polling_interval_in_seconds: number of seconds to wait before polling for
       execution status again
   :type polling_interval_in_seconds: int
   :param overwrite_existing: if the storage bucket + namespace is not empty, it will be
       emptied prior to exports. This enables overwriting existing backups.
   :type overwrite_existing: bool

   .. attribute:: template_fields
      :annotation: = ['bucket', 'namespace', 'entity_filter', 'labels']

      

   
   .. method:: execute(self, context)




.. py:class:: DatastoreImportOperator(bucket:str, file:str, namespace:Optional[str]=None, entity_filter:Optional[dict]=None, labels:Optional[dict]=None, datastore_conn_id:str='google_cloud_default', delegate_to:Optional[str]=None, polling_interval_in_seconds:float=10, project_id:Optional[str]=None, *args, **kwargs)

   Bases: :class:`airflow.models.BaseOperator`

   Import entities from Cloud Storage to Google Cloud Datastore

   :param bucket: container in Cloud Storage to store data
   :type bucket: str
   :param file: path of the backup metadata file in the specified Cloud Storage bucket.
       It should have the extension .overall_export_metadata
   :type file: str
   :param namespace: optional namespace of the backup metadata file in
       the specified Cloud Storage bucket.
   :type namespace: str
   :param entity_filter: description of what data from the project is included in
       the export, refer to
       https://cloud.google.com/datastore/docs/reference/rest/Shared.Types/EntityFilter
   :type entity_filter: dict
   :param labels: client-assigned labels for cloud storage
   :type labels: dict
   :param datastore_conn_id: the name of the connection id to use
   :type datastore_conn_id: str
   :param delegate_to: The account to impersonate, if any.
       For this to work, the service account making the request must have domain-wide
       delegation enabled.
   :type delegate_to: str
   :param polling_interval_in_seconds: number of seconds to wait before polling for
       execution status again
   :type polling_interval_in_seconds: float

   .. attribute:: template_fields
      :annotation: = ['bucket', 'file', 'namespace', 'entity_filter', 'labels']

      

   
   .. method:: execute(self, context)




